{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[E-06]lyrics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "버전 확인"
      ],
      "metadata": {
        "id": "naovNS2k7I44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-wR4CHQH2E6",
        "outputId": "ff7359ae-7add-4ea7-f72b-42b4a907fc03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow \n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 읽어오기"
      ],
      "metadata": {
        "id": "w0a2Nxi67LFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os , re\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "txt_file_path = '/content/drive/MyDrive/aiffel csv/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus =[]\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "        \n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thl-u8bCH8Z9",
        "outputId": "597ab3e3-1324-4330-dde7-ffe560c1dde1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " [\"Let's stay together I, I'm I'm so in love with you\", 'Whatever you want to do', 'Is all right with me']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 정제"
      ],
      "metadata": {
        "id": "rVD2cgltBBZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() #1. 양쪽 공백제거 및 모두 소문자로 변환\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\"  \\1 \", sentence)  #2 특수문자 양쪽 공백 넣기\n",
        "\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  #3 여러개의 공백을 하나의 공백\n",
        "\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #4 a-zA_Z?.!,¿ 가 아닌 모든 문자를 하나의 공백을 바꿈\n",
        "    \n",
        "    sentence = sentence.strip() #5 다시 양쪽 공백을 지움\n",
        "\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start> , 끝에는 <end> 추가\n",
        "\n",
        "    return sentence\n",
        "# 문장 확인\n",
        "print(preprocess_sentence(\" This @_is ;;;sample      Sentence. \"))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVGhBeUx2_tW",
        "outputId": "7e01d674-7f4c-4c64-d10f-e04b11c5cc8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    \n",
        "    if len(sentence) == 0: continue # 우리가 원하지 않는 문장 건너뛰기\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence) \n",
        "    \n",
        "    if len(preprocessed_sentence.split()) > 15: continue  # 토큰의 개수 15개 넘어가는 문장 데이터 제외하기\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5LiPJgM3Min",
        "outputId": "163bd26e-cd2a-4434-fe74-c3981272d84c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156227\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> whatever you want to do <end>',\n",
              " '<start> is all right with me <end>',\n",
              " '<start> cause you make me feel so brand new <end>',\n",
              " '<start> loving you forever <end>',\n",
              " '<start> is what i need <end>',\n",
              " '<start> let me , be the one you come running to <end>',\n",
              " '<start> i ll never be untrue oh baby <end>',\n",
              " '<start> let s , let s stay together gether <end>',\n",
              " '<start> lovin you whether , whether <end>',\n",
              " '<start> times are good or bad , happy or sad <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "\n",
        "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S92QXtLq3N_Z",
        "outputId": "403e3621-fb73-491b-d047-281f2fefd85d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 570   7 ...   0   0   0]\n",
            " [  2  26  25 ...   0   0   0]\n",
            " [  2  66   7 ...   0   0   0]\n",
            " ...\n",
            " [  2  30  19 ...   1   3   0]\n",
            " [  2 344  19 ...   3   0   0]\n",
            " [  2  41 129 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fd5657d3b10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "    \n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Vi1WtR3RGW",
        "outputId": "b5706dd9-eb63-4249-a6b2-be27480023b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련 데이터셋과 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "zT0GSGaI_JYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "src_input = tensor[:, :-1]  #처음~마지막 -1\n",
        "tgt_input = tensor[:, 1:]  #처음+1~마지막\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=30)\n"
      ],
      "metadata": {
        "id": "A0GPyrdv_K5q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source.Train:\", enc_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7RngL4XAR_0",
        "outputId": "1b4faa24-8d30-4d4c-aae8-b4b2f5fdb86c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source.Train: (124981, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input) # source senten 수\n",
        "BATCH_SIZE = 128 # 한 번 학습할 데이터 수\n",
        "\n",
        "\n",
        "steps_per_eopch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1 #왜 1을 더해줬을까나 ,tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
        "# VOCAB_SIZE : 벡터화한 단어 개수\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqhpdM8nAa4B",
        "outputId": "12035881-db32-4a82-b765-7be19d296f27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(128, 14), dtype=tf.int32, name=None), TensorSpec(shape=(128, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 구성"
      ],
      "metadata": {
        "id": "zf4QAGv1-CZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        #embedding_size는 word vector의 차원수이다.\n",
        "        #hidden size는 LSTM레이어의 hidden state의 차원수이다 \n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "embedding_size = 128 \n",
        "hidden_size =256\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
      ],
      "metadata": {
        "id": "4SbSgBytBYXC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "# 데이터셋에서 데이터 한 배치만 불러온다.\n",
        "\n",
        "model(src_sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iMWM-pYDIVO",
        "outputId": "09c92c58-f319-4155-c45a-6dc17ead6fe9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(128, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-6.95941781e-05,  1.74619128e-08, -1.75924433e-04, ...,\n",
              "          1.08085420e-04,  4.28343956e-05,  1.44931866e-04],\n",
              "        [-2.53788749e-04, -5.53594073e-05, -4.82216070e-04, ...,\n",
              "          1.04496954e-04,  5.98956649e-05,  2.57173058e-04],\n",
              "        [-2.83965375e-04, -2.02226583e-04, -6.38117082e-04, ...,\n",
              "          9.31035102e-05, -1.68621639e-07,  2.45723058e-04],\n",
              "        ...,\n",
              "        [ 4.05087689e-04, -1.23412767e-03, -1.14462606e-03, ...,\n",
              "         -1.14924667e-04, -1.41123706e-03,  1.12687564e-03],\n",
              "        [ 6.03283173e-04, -1.39602239e-03, -1.38560834e-03, ...,\n",
              "         -1.10887369e-04, -1.73436734e-03,  1.24715548e-03],\n",
              "        [ 7.76912493e-04, -1.54053012e-03, -1.60550291e-03, ...,\n",
              "         -1.30000262e-04, -2.02825037e-03,  1.34720991e-03]],\n",
              "\n",
              "       [[-6.95941781e-05,  1.74619128e-08, -1.75924433e-04, ...,\n",
              "          1.08085420e-04,  4.28343956e-05,  1.44931866e-04],\n",
              "        [-1.52170396e-04,  1.01160098e-04, -3.23537912e-04, ...,\n",
              "          2.84587895e-05,  2.27801374e-05,  1.26896484e-04],\n",
              "        [-3.22059350e-04,  1.36278613e-04, -2.48699915e-04, ...,\n",
              "          2.30789483e-05,  1.01521131e-04,  1.92109481e-04],\n",
              "        ...,\n",
              "        [ 8.05498566e-04, -9.72583424e-04, -1.59335532e-03, ...,\n",
              "          4.54496127e-04, -1.41849718e-03,  1.03053160e-03],\n",
              "        [ 9.28417256e-04, -1.16337871e-03, -1.84051960e-03, ...,\n",
              "          4.05122613e-04, -1.73213554e-03,  1.14734529e-03],\n",
              "        [ 1.03386596e-03, -1.33191247e-03, -2.05199048e-03, ...,\n",
              "          3.28939816e-04, -2.01666029e-03,  1.24998996e-03]],\n",
              "\n",
              "       [[-6.95941781e-05,  1.74619128e-08, -1.75924433e-04, ...,\n",
              "          1.08085420e-04,  4.28343956e-05,  1.44931866e-04],\n",
              "        [-2.16097978e-05, -1.49451458e-04, -1.86889753e-04, ...,\n",
              "          2.40525333e-04,  5.38107145e-07,  4.02101927e-04],\n",
              "        [ 1.06276362e-04, -2.32405713e-04, -2.24442134e-04, ...,\n",
              "          3.45626730e-04,  4.98437003e-05,  4.55016358e-04],\n",
              "        ...,\n",
              "        [ 5.30555379e-04, -1.05584483e-03, -1.73796201e-03, ...,\n",
              "          4.65079473e-04, -1.15344615e-03,  1.08469056e-03],\n",
              "        [ 7.11637724e-04, -1.14532397e-03, -1.93575269e-03, ...,\n",
              "          4.61568561e-04, -1.39387359e-03,  1.19967107e-03],\n",
              "        [ 8.74344609e-04, -1.24005263e-03, -2.11043004e-03, ...,\n",
              "          4.31175984e-04, -1.63936894e-03,  1.29990955e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-6.95941781e-05,  1.74619128e-08, -1.75924433e-04, ...,\n",
              "          1.08085420e-04,  4.28343956e-05,  1.44931866e-04],\n",
              "        [-1.45011203e-04,  1.40159527e-05, -4.21446020e-04, ...,\n",
              "          4.47459752e-05,  2.11462553e-04,  3.22317937e-04],\n",
              "        [-3.02682165e-04,  3.96795949e-05, -3.49625974e-04, ...,\n",
              "         -1.82197051e-04,  1.57595554e-04,  6.04387431e-04],\n",
              "        ...,\n",
              "        [ 3.56492877e-04, -9.40461832e-05,  3.04637273e-04, ...,\n",
              "          5.98436221e-04,  1.83728173e-06,  2.50973797e-04],\n",
              "        [ 5.22408343e-04, -1.77113718e-04,  3.12856107e-04, ...,\n",
              "          6.71763148e-04,  1.90251274e-04,  5.34981955e-05],\n",
              "        [ 7.28311541e-04, -2.64982460e-04,  5.68657124e-05, ...,\n",
              "          8.00536422e-04,  6.81380843e-05,  5.07449295e-05]],\n",
              "\n",
              "       [[-6.95941490e-05,  1.75075225e-08, -1.75924448e-04, ...,\n",
              "          1.08085449e-04,  4.28344320e-05,  1.44931910e-04],\n",
              "        [-1.62726821e-04,  5.68116920e-05, -3.33579868e-04, ...,\n",
              "          3.00260639e-04,  2.92467827e-04,  2.36393651e-04],\n",
              "        [-2.43075599e-04,  9.92634887e-05, -5.44407289e-04, ...,\n",
              "          2.47276796e-04,  5.73637604e-04,  4.02688835e-04],\n",
              "        ...,\n",
              "        [ 4.74633744e-05, -3.83864884e-04, -4.56676178e-04, ...,\n",
              "         -5.20728703e-04, -5.25839278e-04,  9.40345810e-04],\n",
              "        [ 2.29897181e-04, -5.76813938e-04, -7.11096043e-04, ...,\n",
              "         -4.40745440e-04, -9.23973450e-04,  1.04943127e-03],\n",
              "        [ 4.04994469e-04, -7.70069950e-04, -9.72411828e-04, ...,\n",
              "         -3.82649509e-04, -1.30700832e-03,  1.14868826e-03]],\n",
              "\n",
              "       [[-6.95941490e-05,  1.75075225e-08, -1.75924448e-04, ...,\n",
              "          1.08085449e-04,  4.28344320e-05,  1.44931910e-04],\n",
              "        [-5.22797345e-05,  4.91057181e-05,  6.85813138e-05, ...,\n",
              "          1.66044672e-04, -1.71174252e-04,  1.88464968e-04],\n",
              "        [ 1.16306364e-04,  5.72959252e-05, -4.56328598e-05, ...,\n",
              "          1.11914240e-04, -2.01748626e-04,  2.14928383e-04],\n",
              "        ...,\n",
              "        [-1.00188088e-04, -6.07832451e-04,  4.72698797e-04, ...,\n",
              "         -1.40519682e-04,  1.24362414e-04, -7.54586596e-04],\n",
              "        [ 2.66605257e-05, -7.28495652e-04,  1.90740233e-04, ...,\n",
              "         -1.28585452e-04, -8.11429927e-05, -5.05857286e-04],\n",
              "        [ 1.69361403e-04, -8.54131067e-04, -1.29125023e-04, ...,\n",
              "         -7.84196964e-05, -3.63781233e-04, -2.24558360e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unXme0k8Djzw",
        "outputId": "636f42b1-202f-4599-cc98-1a49ed163a75"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  1536128   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  394240    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  525312    \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  3084257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,539,937\n",
            "Trainable params: 5,539,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 학습"
      ],
      "metadata": {
        "id": "_6JCAo0K-YsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "history = model.fit(dataset, epochs=1, validation_data = (enc_val, dec_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPNh_UmvDtuu",
        "outputId": "1f990957-4a4c-4170-8888-2f539758c18d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1220/1220 [==============================] - 1332s 1s/step - loss: 3.6818 - val_loss: 3.3281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "caggle pro를 쓰지만!!!!!!!!!!!!!!!!!!!!!!!! 노트북 사양이 너무 구린 나머지..... 임베딩사이즈랑 히든사이즈를 처음에 256 1024 를 했지만 너무 오래걸려서 값을 바꾸었고 epochs도 1로 했습니다 아마\n",
        "제대로 했다면 손실값이 2.2 아래로 나왔을 겁니다!!! \n",
        " "
      ],
      "metadata": {
        "id": "Cf_NlzN4-qYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델평가"
      ],
      "metadata": {
        "id": "hwX57Qk-_ZhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "FjQn3XsEkshS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1uXkzKUgJWuE",
        "outputId": "b5fe6322-b700-474f-b3fe-e24c769e7d81"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> love , i t you you you you you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> babe\", max_len=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cT3Z43B3_fgc",
        "outputId": "4d90ef94-13a7-4e77-e8cf-2b4ceff5f7bc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> babe , i i t you you you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ho8v_BFp_0wr",
        "outputId": "b23524be-229c-4434-f851-0eb2f0d0c49e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> you i t t you you you you you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xHaHHerc_4ux",
        "outputId": "d3bec5cc-6cf0-43eb-d417-deed737ab133"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i i t t you you you you you you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you 만 나오는 진기한 광경이 펼쳐 지고있군요...."
      ],
      "metadata": {
        "id": "_UW21cMg_8ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#회고\n",
        "\n",
        "초반에 뛰어쓰기 하나때문에 몇분정도 막혔지만 \n",
        "성돈님과 함께 원인을 찾아 다행이 해결했지만 \n",
        "\n",
        "결과가 좋지못해 아쉬웠다. \n",
        "\n",
        "노트북 바꾸고싶다 정말 \n",
        "\n",
        " 쪼금 재미가 있었다1"
      ],
      "metadata": {
        "id": "vaQ0DpNWAJ4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YF2Nf6ABA7cQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}